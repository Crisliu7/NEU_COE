{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Research Project Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "October 5, 2017 – Research project proposals due.\n",
    "\n",
    "**What is expected?**\n",
    "\n",
    "a.\tA clear description of the topic.  \n",
    "b.\tBackground research of related work.  \n",
    "c.\tData sources?  \n",
    "d.\tWhat algorithms are being used and code sources.  \n",
    "e.\tReferences.  \n",
    "\n",
    "See the example research project proposals.  \n",
    "\n",
    "**Group Size**\n",
    "\n",
    "One to two people.\n",
    "\n",
    "**Progress reports:**\n",
    "\n",
    "Research project progress reports will be due December 9, 2017\n",
    "\n",
    "A draft of the final report written in a scientific paper format.\n",
    "\n",
    "The project progress reports must have:\n",
    "\n",
    "a.\tAbstract (10 %)  \n",
    "b.\tIntroduction (5 %)  \n",
    "c.\tCode with Documentation (50%)  \n",
    "d.\tResults (20 %)  \n",
    "e.\tDiscussion (10 %)  \n",
    "f.\tReferences (5 %)   \n",
    "\n",
    "\n",
    "**Projects Due:**\n",
    "\n",
    "Research project code and reports will be due December 13, 2015.\n",
    "\n",
    "What is expected?\n",
    "\n",
    "A final draft of the research written in a scientific paper format. You are expected to respond to feedback you received from the progress report. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Grading Rubric:**\n",
    "\n",
    "The following breakdown will be used for determining the score for the research project: \n",
    "\n",
    "| Assignment              | Points   |\n",
    "|-------------------------|----------|\n",
    "| Project proposals       |      100 |\n",
    "| Progress report         |      150 |\n",
    "| Progress draft          |      250 |\n",
    "| Research project paper  |      500 |\n",
    "\n",
    "\t\n",
    "\n",
    "**Research Projects**\n",
    "\n",
    "This course will have individual machine learning research papers.  These papers should be in a style that could be submitted to a conference, workshop or journal. Students should begin to work on this in early in the semester.\n",
    "\n",
    "These assignments will provide practice in real-world analysis and application of machine learning algorithms. The research can take one of the following forms:\n",
    "\n",
    "i.\tTweaking an existing machine learning algorithm.  \n",
    "ii.\tApplying an existing machine learning algorithm in a novel context.  \n",
    "iii.\tValidating an existing machine learning algorithm in real-world contexts.  \n",
    "iv.\tCreating a novel machine learning algorithm.  \n",
    "v. Competing in a compeition like Kaggle [https://www.kaggle.com/](https://www.kaggle.com/)   \n",
    "\n",
    "_Topic 1_\n",
    "\n",
    "Tweaking an existing algorithm \n",
    "This project involves finding an existing algorithm and making a modification that might improve its recall, precision, scalability, memory usage or speed. Other tweaks may include changing the algorithm so it applies in a different context.  \n",
    "\n",
    "_Topic 2_\n",
    "\n",
    "Applying an existing algorithm in a novel context\n",
    "This project involves finding an existing algorithm and applying it in a novel way. This might involve applying genetic sequencing algorithms to the optimization of hardware resource sharing, or the use of genetic algorithms (GAs) to optimize the arrangement of class schedules.  This topic is very closely related to “Tweaking an existing algorithm” in that one usually needs to make changes to use an algorithm in a novel context. \n",
    "\n",
    "_Topic 3_\n",
    "\n",
    "Validating an existing algorithm in real-world contexts \n",
    "This project involves finding how practically useful and applicable an existing algorithm really is in the “real world.” This involves empirically validating the sensitivity, recall, precision, scalability, memory usage and speed claims of an algorithm with realistic, noisy and non-ideal data.\n",
    "\n",
    "_Topic 4_\n",
    "\n",
    "Creating a novel algorithm \n",
    "This project involves creating a novel algorithm that can answer an interesting real-world question. This topic is very closely related to “Tweaking an existing algorithm” in that one usually extends and improves what exists rather than create totally from scratch.\n",
    "\n",
    "_Topic versus Thesis_\n",
    "\n",
    "A topic is a general interest. A thesis statement presents an assertion; what you intend to do and how you intend to prove/convince others what you did is correct.  A topic of interest might be “What keywords should I add to my tweet to make it more viral?” \n",
    "\n",
    "A thesis is more specific and is framed such that the assertion is testable.  A thesis would be “We believe adding keywords according to our algorithm will significantly improve a tweets virality.”  The paper would then involve quantitatively defining virality and comparing the random tweets and the tweaked tweets with the keyword algorithm using reasonable real-world data (e.g. Twitter).\n",
    "\n",
    "\n",
    "**Submission **\n",
    "You will submit your assignments via BlackBoard.\n",
    "Click the title of assignment (blackboard -> assignment -> <Title of Assignment>), to go to the submission page. \n",
    "\n",
    "**What Topics are Interesting?**\n",
    "•\tSequence alignment\n",
    "•\tSecure Communication\n",
    "•\tScheduling (courses, trains, etc.)\n",
    "•\tPacking (where to best place items in store/website/warehouse)\n",
    "•\tPath finding (who/what is closet? How do I get there? Extracting a Retweet’s Origins?)\n",
    "•\tWhat terms are associated with your Twitter/ Facebook/ LinkedIn/ Google+:/ GitHub/ Web handle?\n",
    "•\tWhich genes cause cancer?\n",
    "•\tWhat users are associated with your Twitter/ Facebook/ LinkedIn/ Google+:/ GitHub/ Web handle?\n",
    "•\tFriendship cliques on Twitter/ Facebook/ LinkedIn/ Google+:/ GitHub/ Web\n",
    "•\tSentiment analysis/influence analysis. Who are most influential on Twitter/ Facebook/ LinkedIn/ Google+:/ GitHub/ Web?\n",
    "•\tMapping (jobs, housing, crime, etc.)\n",
    "•\tAt what price should I start an Ebay auction?\n",
    "•\tDo I want to follow that person back?\n",
    "•\tWhere should I place transmission towers?\n",
    "•\tWhat keywords should I add to my tweet/post?\n",
    "•\tIdentification of Transcription Factor Binding Sites\n",
    "•\tWhen should I tweet/post?\n",
    "•\tMatching (Who is like/unlike me? What is the best TV, college, etc. for me?)\n",
    "•\tWhat is the reach of my tweet/post?\n",
    "•\tWhat are people saying about me on Twitter/ Facebook/ LinkedIn/ Google+:/ GitHub/ Web?\n",
    "•\tShould I add a picture/url to my tweet/post?\n",
    "\n",
    "There are an infinite number of interesting topics to which machine learning algorithms could be applied.  You want to keep your topic simple and doable. A topic like “Which genes cause cancer?” is of great interest but too broad for a topic. Analyzing existing gene expression quantification algorithms is more suitable for a project. The purpose of these projects is for students do get their “hands dirty,” not to necessarily develop break-through algorithms.  We will discuss how to do this research early in the semester.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example projects\n",
    "\n",
    "There are several example projects and papers on NEU's BlackBoard for this course.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Special Project in Deep Learning\n",
    "\n",
    "Students who wish can contribute to a larger class project rather to the small group or individual assignments.\n",
    "\n",
    "The description of the project is below:\n",
    "\n",
    "**Understanding the semantics of the latent space in unsupervised deep learning models**\n",
    "\n",
    "**Abstract:**\n",
    "\n",
    "Deep learning and neural networks are increasingly important concepts as demonstrated through their performance on difficult problems in computer vision, medical diagnosis, natural language processing and many other domains. Deep learning algorithms are unique in that they try to learn latent features from data, as opposed to traditional machine learning where features selection is typically handcrafted. However, the semantics of deep neural networks “hidden layers” are poorly understood, and are often treated as “black box” models.  The aim of this research is to develop tools and algorithms to better understand the semantics of the latent features learned by deep networks, particularly those used for unsupervised deep learning.\n",
    "\n",
    "**Unsupervised models to be studied:**\n",
    "\n",
    "_Autoencoders_\n",
    "\n",
    "An autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.\n",
    "\n",
    "Autoencoder’s though are difficult to interpret of the representation of semantics and aren’t really a generative model.\n",
    "\n",
    "Autoencoder\n",
    "i.\tencoder and dencoder  \n",
    "ii.\thidden layers\n",
    "iii.\tbottleneck layer - forces network to learn a compressed latent representation\n",
    "iv.\treconstruction loss  - forces hidden layer to represent info about the input\n",
    "\n",
    "\n",
    "_Variational autoencoders (VAEs)_\n",
    "\n",
    "Variational autoencoders are a stochastic variational extension of autoencoders  that allow for a probabilistic representation of the data and amortized inference.\n",
    "In a VAE, the encoder becomes a variational inference network that maps the data to the a distribution for the hidden variables, and the decoder becomes a generative network that maps the latent variables back to the data. In just a couple of years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions.\n",
    "\n",
    "Variational Autoencoders (VAEs) have some very desirable properties:\n",
    "\n",
    "i.\tClear semantics as a generative model\n",
    "ii.\tExtension of autoencoders that allow sampling and estimating probabilities.\n",
    "iii.\tThis creates a kind of implicit generative model it creates a latent representation thru its probabilities\n",
    "iv.\t\"Latent variables\" has a fixed prior distribution\n",
    "v.\tProbabilistic encoder and dencoder\n",
    "vi.\tProbabilistic representation of the data\n",
    "\n",
    "Variational autoencoders have a clear advantage over autoencoders in that the probabilistic representation of the data is a form of representation learning.\n",
    "\n",
    "\n",
    "_Autoregresstive variational autoencoders_\n",
    "\n",
    "Standard VAE’s have some issues:\n",
    "i.\tThey do not encode what is not useful for them to decode.  This means they don't capture fine details. Subtle features (e.g. small nodules that may be important clinically but don’t represent much of the image space may be missed.)\n",
    "\n",
    "\n",
    "Autoregresstive VAE’s use autoregresstive networks in the encoder and the dencoder to capture more local info in the estimation of the densities.\n",
    "\n",
    "\n",
    "_Restricted Boltzmann machines (RBMs)_\n",
    "\n",
    "Restricted Boltzmann machines (RBM)s are a very simple model, just a fully connected bipartite graph with an input layer and a hidden layer. The cost function minimizes an energy function by re-weighting to minimize the difference between the input layer and the hidden layer.\n",
    "\n",
    "\n",
    "Like VAE’s restricted Boltzmann machines can be thought as a form of representation learning.\n",
    "\n",
    "Restricted Boltzmann machines:\n",
    "i.\tClear semantics as a generative model\n",
    "ii.\tSimple, just a fully connected bipartite graph with an input layer and a hidden layer.\n",
    "iii.\tMinimizes an energy function (re-weighting to minimize the difference between the input layer and the hidden layer).\n",
    "iv.\tEnergies can easily be converted to probabilities.\n",
    "v.\tCreates a latent representation thru its probabilities.\n",
    "\n",
    "Like VAE’s, the densities generated can be visualized.  When run on image data sets of numbers, the visualizations clearly show where the densities show portions of 2’s, 3’s, 7’s. etc.\n",
    "\n",
    "\n",
    "**Understanding the semantics of the latent space**\n",
    "\n",
    "The research includes implementing models to help understand the semantics of the latent space in VAEs and RBMs. This includes:\n",
    "\n",
    "A. Visualizing the latent space\n",
    "\n",
    "i.\tThe densities generated can be visualized.  \n",
    "ii.\tVisualizing these data using t-SNE.\n",
    "iii.\tGenerative models are also used to visualize the semantic meaning of hidden layers. To visualize the semantic meaning of each layers of generative models, the parameters of the models can be gradually adjusted, and the effect on generated images are observed.\n",
    "\n",
    "Latent space visualization — Deep Learning bits #2 [https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df](https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df)\n",
    "\n",
    "Discovering Hidden Factors of Variation in Deep Networks [https://arxiv.org/abs/1412.6583](https://arxiv.org/abs/1412.6583)\n",
    "\n",
    "Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014, pp. 818–833. Springer, 2014. [https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n",
    "\n",
    "Topic Modeling and t-SNE Visualization [https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html](https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html)\n",
    "\n",
    "Visualizing Data Using t-SNE [https://www.youtube.com/watch?v=RJVL80Gg3lA](https://www.youtube.com/watch?v=RJVL80Gg3lA)\n",
    "\n",
    "Visualizing data using t-SNE [https://www.researchgate.net/publication/228339739_Viualizing_data_using_t-SNE](https://www.researchgate.net/publication/228339739_Viualizing_data_using_t-SNE)\n",
    "\n",
    "\n",
    "B. Ranking the latent space\n",
    "\n",
    "Machine learning latent factor models such as singular value decomposition (SVD), principal component analysis (PCA), and probabilistic PCA (PPCA) have the very powerful property that the first k components or first k terms can be ranked. This is very desirable, it allows for:\n",
    "\n",
    "i.\tA quantitative assessment of signal loss.\n",
    "ii.\tDimensionality reduction.\n",
    "iii.\tA powerful form of regularization by removing the components or terms that contributes very to the signal.\n",
    "\n",
    "I’ve had a hard time finding papers related to ranking the latent space in deep learning.\n",
    "\n",
    "So far just,\n",
    "\n",
    "Deep Variational Canonical Correlation Analysis [https://arxiv.org/abs/1610.03454](https://arxiv.org/abs/1610.03454)\n",
    "\n",
    "C. Latent space arithmetics\n",
    "\n",
    "Another way of exploring the learned representations is to show arithmetic in the latent space. Generative nets have been shown to encode semantic knowledge of such things as glasses, chair or face images in its latent space. One can “add” or “subtract” latent encoding and the effect on generated images are observed. Like adding glasses to a generated human face, in theory one could add nodules to a generated human lung.\n",
    "\n",
    "\n",
    "Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015.   [https://arxiv.org/abs/1411.5928](https://arxiv.org/abs/1411.5928)\n",
    "\n",
    "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. [](https://arxiv.org/abs/1511.06434\n",
    "\n",
    "Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and generative vector representation for objects. In ECCV, 2016. [https://arxiv.org/abs/1603.08637](https://arxiv.org/abs/1603.08637)\n",
    "\n",
    "**Student responsibilities:**\n",
    "\n",
    "a.\tImplement models specified by Professor Brown in Keras and TensorFlow  \n",
    "b.\tBi-weekly progress updates (once every two weeks)   \n",
    "c.\tDiscuss results from models with Professor Brown   \n",
    "d.\tShould interesting results arise co-author papers with Professor Brown   \n",
    "e.  Think of novel ideas and approaches and suggest them to the working group     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of datasets for machine learning research\n",
    "\n",
    "* [List of datasets for machine learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)   \n",
    "* [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/)  \n",
    "* [Public Data Sets : Amazon Web Services](https://aws.amazon.com/datasets/) \n",
    "* [freebase](https://developers.google.com/freebase/)  \n",
    "* [Google Public Data Explorer](https://www.google.com/publicdata/directory)  \n",
    "* [datahub](http://datahub.io/)  \n",
    "* [data.gov](https://www.data.gov/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do a literature review\n",
    "\n",
    "* [http://www.asbmb.org/asbmbtoday/asbmbtoday_article.aspx?id=15161](http://www.asbmb.org/asbmbtoday/asbmbtoday_article.aspx?id=15161)  \n",
    "* [http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003149](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003149)  \n",
    "* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3715443/](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3715443/)  \n",
    "* [http://www.monash.edu.au/lls/llonline/writing/science/lit-review/1.xml](http://www.monash.edu.au/lls/llonline/writing/science/lit-review/1.xml)  \n",
    "* [http://library.lincoln.ac.nz/Research/Writing-your-research/Literature-Reviews/Sample-literature-reviews/](http://library.lincoln.ac.nz/Research/Writing-your-research/Literature-Reviews/Sample-literature-reviews/)  \n",
    "* [http://www.coe.montana.edu/ee/rosss/Courses/EE578_Fall_2008/Writing%20a%20Review%20Paper.pdf](http://www.coe.montana.edu/ee/rosss/Courses/EE578_Fall_2008/Writing%20a%20Review%20Paper.pdf)  \n",
    "* [http://writingcenter.unc.edu/handouts/literature-reviews/](http://writingcenter.unc.edu/handouts/literature-reviews/)  \n",
    "* [http://writing.wisc.edu/Handbook/ReviewofLiterature.html](http://writing.wisc.edu/Handbook/ReviewofLiterature.html)  \n",
    "* [http://guides.library.ucsc.edu/write-a-literature-review](http://guides.library.ucsc.edu/write-a-literature-review)  \n",
    "* [http://www.duluth.umn.edu/~hrallis/guides/researching/litreview.html](http://www.duluth.umn.edu/~hrallis/guides/researching/litreview.html)  \n",
    "* [https://ithacalibrary.com/sp/assets/users/_lchabot/lit_rev_eg.pdf](https://ithacalibrary.com/sp/assets/users/_lchabot/lit_rev_eg.pdf)  \n",
    "* [http://www.lib.ncsu.edu/tutorials/litreview/](http://www.lib.ncsu.edu/tutorials/litreview/)   \n",
    "* [http://library.concordia.ca/help/howto/litreview.php](http://library.concordia.ca/help/howto/litreview.php)  \n",
    "* [http://library.bcu.ac.uk/learner/writingguides/1.04.htm](http://library.bcu.ac.uk/learner/writingguides/1.04.htm)  \n",
    "* [http://guides.library.vcu.edu/lit-review](http://guides.library.vcu.edu/lit-review)  \n",
    "* [http://www2.le.ac.uk/offices/ld/resources/writing/writing-resources/literature-review](http://www2.le.ac.uk/offices/ld/resources/writing/writing-resources/literature-review)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update September 5, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
